{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename,encoding= 'UTF-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163817\n",
      "Total Vocab:  61\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163717\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.8344Epoch 00001: loss improved from inf to 2.83442, saving model to Weights-LSTM1-improvement-01-2.8344-bigger.hdf5\n",
      "163717/163717 [==============================] - 778s 5ms/step - loss: 2.8344\n",
      "Epoch 2/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.5155Epoch 00002: loss improved from 2.83442 to 2.51550, saving model to Weights-LSTM1-improvement-02-2.5155-bigger.hdf5\n",
      "163717/163717 [==============================] - 776s 5ms/step - loss: 2.5155\n",
      "Epoch 3/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.3197Epoch 00003: loss improved from 2.51550 to 2.31967, saving model to Weights-LSTM1-improvement-03-2.3197-bigger.hdf5\n",
      "163717/163717 [==============================] - 823s 5ms/step - loss: 2.3197\n",
      "Epoch 4/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.1825Epoch 00004: loss improved from 2.31967 to 2.18252, saving model to Weights-LSTM1-improvement-04-2.1825-bigger.hdf5\n",
      "163717/163717 [==============================] - 865s 5ms/step - loss: 2.1825\n",
      "Epoch 5/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.0804Epoch 00005: loss improved from 2.18252 to 2.08041, saving model to Weights-LSTM1-improvement-05-2.0804-bigger.hdf5\n",
      "163717/163717 [==============================] - 904s 6ms/step - loss: 2.0804\n",
      "Epoch 6/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.9982Epoch 00006: loss improved from 2.08041 to 1.99823, saving model to Weights-LSTM1-improvement-06-1.9982-bigger.hdf5\n",
      "163717/163717 [==============================] - 833s 5ms/step - loss: 1.9982\n",
      "Epoch 7/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.9325Epoch 00007: loss improved from 1.99823 to 1.93251, saving model to Weights-LSTM1-improvement-07-1.9325-bigger.hdf5\n",
      "163717/163717 [==============================] - 816s 5ms/step - loss: 1.9325\n",
      "Epoch 8/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.8795Epoch 00008: loss improved from 1.93251 to 1.87951, saving model to Weights-LSTM1-improvement-08-1.8795-bigger.hdf5\n",
      "163717/163717 [==============================] - 1170s 7ms/step - loss: 1.8795\n",
      "Epoch 9/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.8287Epoch 00009: loss improved from 1.87951 to 1.82865, saving model to Weights-LSTM1-improvement-09-1.8286-bigger.hdf5\n",
      "163717/163717 [==============================] - 2025s 12ms/step - loss: 1.8286\n",
      "Epoch 10/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.7895Epoch 00010: loss improved from 1.82865 to 1.78953, saving model to Weights-LSTM1-improvement-10-1.7895-bigger.hdf5\n",
      "163717/163717 [==============================] - 1141s 7ms/step - loss: 1.7895\n",
      "Epoch 11/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.7528Epoch 00011: loss improved from 1.78953 to 1.75277, saving model to Weights-LSTM1-improvement-11-1.7528-bigger.hdf5\n",
      "163717/163717 [==============================] - 802s 5ms/step - loss: 1.7528\n",
      "Epoch 12/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.7159Epoch 00012: loss improved from 1.75277 to 1.71585, saving model to Weights-LSTM1-improvement-12-1.7159-bigger.hdf5\n",
      "163717/163717 [==============================] - 795s 5ms/step - loss: 1.7159\n",
      "Epoch 13/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.6899Epoch 00013: loss improved from 1.71585 to 1.68991, saving model to Weights-LSTM1-improvement-13-1.6899-bigger.hdf5\n",
      "163717/163717 [==============================] - 783s 5ms/step - loss: 1.6899\n",
      "Epoch 14/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.6621Epoch 00014: loss improved from 1.68991 to 1.66211, saving model to Weights-LSTM1-improvement-14-1.6621-bigger.hdf5\n",
      "163717/163717 [==============================] - 786s 5ms/step - loss: 1.6621\n",
      "Epoch 15/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.6371Epoch 00015: loss improved from 1.66211 to 1.63708, saving model to Weights-LSTM1-improvement-15-1.6371-bigger.hdf5\n",
      "163717/163717 [==============================] - 787s 5ms/step - loss: 1.6371\n",
      "Epoch 16/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.6154Epoch 00016: loss improved from 1.63708 to 1.61535, saving model to Weights-LSTM1-improvement-16-1.6153-bigger.hdf5\n",
      "163717/163717 [==============================] - 783s 5ms/step - loss: 1.6153\n",
      "Epoch 17/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5947Epoch 00017: loss improved from 1.61535 to 1.59471, saving model to Weights-LSTM1-improvement-17-1.5947-bigger.hdf5\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.5947\n",
      "Epoch 18/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5768Epoch 00018: loss improved from 1.59471 to 1.57679, saving model to Weights-LSTM1-improvement-18-1.5768-bigger.hdf5\n",
      "163717/163717 [==============================] - 786s 5ms/step - loss: 1.5768\n",
      "Epoch 19/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5602Epoch 00019: loss improved from 1.57679 to 1.56016, saving model to Weights-LSTM1-improvement-19-1.5602-bigger.hdf5\n",
      "163717/163717 [==============================] - 791s 5ms/step - loss: 1.5602\n",
      "Epoch 20/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5434Epoch 00020: loss improved from 1.56016 to 1.54345, saving model to Weights-LSTM1-improvement-20-1.5435-bigger.hdf5\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.5435\n",
      "Epoch 21/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5315Epoch 00021: loss improved from 1.54345 to 1.53157, saving model to Weights-LSTM1-improvement-21-1.5316-bigger.hdf5\n",
      "163717/163717 [==============================] - 787s 5ms/step - loss: 1.5316\n",
      "Epoch 22/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5184Epoch 00022: loss improved from 1.53157 to 1.51842, saving model to Weights-LSTM1-improvement-22-1.5184-bigger.hdf5\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.5184\n",
      "Epoch 23/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5102Epoch 00023: loss improved from 1.51842 to 1.51014, saving model to Weights-LSTM1-improvement-23-1.5101-bigger.hdf5\n",
      "163717/163717 [==============================] - 788s 5ms/step - loss: 1.5101\n",
      "Epoch 24/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4947Epoch 00024: loss improved from 1.51014 to 1.49471, saving model to Weights-LSTM1-improvement-24-1.4947-bigger.hdf5\n",
      "163717/163717 [==============================] - 791s 5ms/step - loss: 1.4947\n",
      "Epoch 25/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4842Epoch 00025: loss improved from 1.49471 to 1.48426, saving model to Weights-LSTM1-improvement-25-1.4843-bigger.hdf5\n",
      "163717/163717 [==============================] - 786s 5ms/step - loss: 1.4843\n",
      "Epoch 26/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4752Epoch 00026: loss improved from 1.48426 to 1.47527, saving model to Weights-LSTM1-improvement-26-1.4753-bigger.hdf5\n",
      "163717/163717 [==============================] - 788s 5ms/step - loss: 1.4753\n",
      "Epoch 27/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4633Epoch 00027: loss improved from 1.47527 to 1.46329, saving model to Weights-LSTM1-improvement-27-1.4633-bigger.hdf5\n",
      "163717/163717 [==============================] - 785s 5ms/step - loss: 1.4633\n",
      "Epoch 28/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4600Epoch 00028: loss improved from 1.46329 to 1.45998, saving model to Weights-LSTM1-improvement-28-1.4600-bigger.hdf5\n",
      "163717/163717 [==============================] - 796s 5ms/step - loss: 1.4600\n",
      "Epoch 29/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4470Epoch 00029: loss improved from 1.45998 to 1.44706, saving model to Weights-LSTM1-improvement-29-1.4471-bigger.hdf5\n",
      "163717/163717 [==============================] - 795s 5ms/step - loss: 1.4471\n",
      "Epoch 30/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4955Epoch 00030: loss did not improve\n",
      "163717/163717 [==============================] - 791s 5ms/step - loss: 1.4955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4389Epoch 00031: loss improved from 1.44706 to 1.43893, saving model to Weights-LSTM1-improvement-31-1.4389-bigger.hdf5\n",
      "163717/163717 [==============================] - 780s 5ms/step - loss: 1.4389\n",
      "Epoch 32/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4232Epoch 00032: loss improved from 1.43893 to 1.42326, saving model to Weights-LSTM1-improvement-32-1.4233-bigger.hdf5\n",
      "163717/163717 [==============================] - 777s 5ms/step - loss: 1.4233\n",
      "Epoch 33/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4185Epoch 00033: loss improved from 1.42326 to 1.41850, saving model to Weights-LSTM1-improvement-33-1.4185-bigger.hdf5\n",
      "163717/163717 [==============================] - 781s 5ms/step - loss: 1.4185\n",
      "Epoch 34/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4165Epoch 00034: loss improved from 1.41850 to 1.41652, saving model to Weights-LSTM1-improvement-34-1.4165-bigger.hdf5\n",
      "163717/163717 [==============================] - 779s 5ms/step - loss: 1.4165\n",
      "Epoch 35/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4142Epoch 00035: loss improved from 1.41652 to 1.41421, saving model to Weights-LSTM1-improvement-35-1.4142-bigger.hdf5\n",
      "163717/163717 [==============================] - 781s 5ms/step - loss: 1.4142\n",
      "Epoch 36/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4099Epoch 00036: loss improved from 1.41421 to 1.40998, saving model to Weights-LSTM1-improvement-36-1.4100-bigger.hdf5\n",
      "163717/163717 [==============================] - 779s 5ms/step - loss: 1.4100\n",
      "Epoch 37/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4095Epoch 00037: loss improved from 1.40998 to 1.40953, saving model to Weights-LSTM1-improvement-37-1.4095-bigger.hdf5\n",
      "163717/163717 [==============================] - 780s 5ms/step - loss: 1.4095\n",
      "Epoch 38/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4058Epoch 00038: loss improved from 1.40953 to 1.40577, saving model to Weights-LSTM1-improvement-38-1.4058-bigger.hdf5\n",
      "163717/163717 [==============================] - 783s 5ms/step - loss: 1.4058\n",
      "Epoch 39/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4085Epoch 00039: loss did not improve\n",
      "163717/163717 [==============================] - 775s 5ms/step - loss: 1.4084\n",
      "Epoch 40/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4067Epoch 00040: loss did not improve\n",
      "163717/163717 [==============================] - 782s 5ms/step - loss: 1.4067\n",
      "Epoch 41/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4042Epoch 00041: loss improved from 1.40577 to 1.40420, saving model to Weights-LSTM1-improvement-41-1.4042-bigger.hdf5\n",
      "163717/163717 [==============================] - 781s 5ms/step - loss: 1.4042\n",
      "Epoch 42/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.3935Epoch 00042: loss improved from 1.40420 to 1.39354, saving model to Weights-LSTM1-improvement-42-1.3935-bigger.hdf5\n",
      "163717/163717 [==============================] - 785s 5ms/step - loss: 1.3935\n",
      "Epoch 43/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.3941Epoch 00043: loss did not improve\n",
      "163717/163717 [==============================] - 955s 6ms/step - loss: 1.3941\n",
      "Epoch 44/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4086Epoch 00044: loss did not improve\n",
      "163717/163717 [==============================] - 1848s 11ms/step - loss: 1.4087\n",
      "Epoch 45/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.3910Epoch 00045: loss improved from 1.39354 to 1.39103, saving model to Weights-LSTM1-improvement-45-1.3910-bigger.hdf5\n",
      "163717/163717 [==============================] - 4984s 30ms/step - loss: 1.3910\n",
      "Epoch 46/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.3956Epoch 00046: loss did not improve\n",
      "163717/163717 [==============================] - 821s 5ms/step - loss: 1.3956\n",
      "Epoch 47/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4384Epoch 00047: loss did not improve\n",
      "163717/163717 [==============================] - 857s 5ms/step - loss: 1.4384\n",
      "Epoch 48/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.3864Epoch 00048: loss improved from 1.39103 to 1.38643, saving model to Weights-LSTM1-improvement-48-1.3864-bigger.hdf5\n",
      "163717/163717 [==============================] - 790s 5ms/step - loss: 1.3864\n",
      "Epoch 49/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4144Epoch 00049: loss did not improve\n",
      "163717/163717 [==============================] - 788s 5ms/step - loss: 1.4144\n",
      "Epoch 50/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4175Epoch 00050: loss did not improve\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.4175\n",
      "Epoch 51/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4038Epoch 00051: loss did not improve\n",
      "163717/163717 [==============================] - 793s 5ms/step - loss: 1.4038\n",
      "Epoch 52/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4477Epoch 00052: loss did not improve\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.4477\n",
      "Epoch 53/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4055Epoch 00053: loss did not improve\n",
      "163717/163717 [==============================] - 791s 5ms/step - loss: 1.4055\n",
      "Epoch 54/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.7913Epoch 00054: loss did not improve\n",
      "163717/163717 [==============================] - 790s 5ms/step - loss: 1.7913\n",
      "Epoch 55/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5556Epoch 00055: loss did not improve\n",
      "163717/163717 [==============================] - 790s 5ms/step - loss: 1.5555\n",
      "Epoch 56/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4555Epoch 00056: loss did not improve\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.4555\n",
      "Epoch 57/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4315Epoch 00057: loss did not improve\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.4315\n",
      "Epoch 58/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4417Epoch 00058: loss did not improve\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.4417\n",
      "Epoch 59/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4248Epoch 00059: loss did not improve\n",
      "163717/163717 [==============================] - 792s 5ms/step - loss: 1.4249\n",
      "Epoch 60/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4483Epoch 00060: loss did not improve\n",
      "163717/163717 [==============================] - 788s 5ms/step - loss: 1.4483\n",
      "Epoch 61/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4672Epoch 00061: loss did not improve\n",
      "163717/163717 [==============================] - 788s 5ms/step - loss: 1.4672\n",
      "Epoch 62/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4361Epoch 00062: loss did not improve\n",
      "163717/163717 [==============================] - 787s 5ms/step - loss: 1.4362\n",
      "Epoch 63/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4534Epoch 00063: loss did not improve\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.4534\n",
      "Epoch 64/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4452Epoch 00064: loss did not improve\n",
      "163717/163717 [==============================] - 788s 5ms/step - loss: 1.4452\n",
      "Epoch 65/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.4233Epoch 00065: loss did not improve\n",
      "163717/163717 [==============================] - 794s 5ms/step - loss: 2.4233\n",
      "Epoch 66/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 2.4936Epoch 00066: loss did not improve\n",
      "163717/163717 [==============================] - 790s 5ms/step - loss: 2.4936\n",
      "Epoch 67/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5223Epoch 00067: loss did not improve\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.5222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4817Epoch 00068: loss did not improve\n",
      "163717/163717 [==============================] - 785s 5ms/step - loss: 1.4817\n",
      "Epoch 69/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.9854Epoch 00069: loss did not improve\n",
      "163717/163717 [==============================] - 782s 5ms/step - loss: 1.9854\n",
      "Epoch 70/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4637Epoch 00070: loss did not improve\n",
      "163717/163717 [==============================] - 783s 5ms/step - loss: 1.4637\n",
      "Epoch 71/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4448Epoch 00071: loss did not improve\n",
      "163717/163717 [==============================] - 784s 5ms/step - loss: 1.4448\n",
      "Epoch 72/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4903Epoch 00072: loss did not improve\n",
      "163717/163717 [==============================] - 784s 5ms/step - loss: 1.4903\n",
      "Epoch 73/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5839Epoch 00073: loss did not improve\n",
      "163717/163717 [==============================] - 789s 5ms/step - loss: 1.5839\n",
      "Epoch 74/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.5009Epoch 00074: loss did not improve\n",
      "163717/163717 [==============================] - 783s 5ms/step - loss: 1.5009\n",
      "Epoch 75/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4497Epoch 00075: loss did not improve\n",
      "163717/163717 [==============================] - 783s 5ms/step - loss: 1.4496\n",
      "Epoch 76/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4695Epoch 00076: loss did not improve\n",
      "163717/163717 [==============================] - 784s 5ms/step - loss: 1.4696\n",
      "Epoch 77/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.4752Epoch 00077: loss did not improve\n",
      "163717/163717 [==============================] - 783s 5ms/step - loss: 1.4752\n",
      "Epoch 78/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.7717Epoch 00078: loss did not improve\n",
      "163717/163717 [==============================] - 783s 5ms/step - loss: 1.7717\n",
      "Epoch 79/80\n",
      "163712/163717 [============================>.] - ETA: 0s - loss: 1.6433Epoch 00079: loss did not improve\n",
      "163717/163717 [==============================] - 788s 5ms/step - loss: 1.6433\n",
      "Epoch 80/80\n",
      " 15168/163717 [=>............................] - ETA: 12:09 - loss: 1.5640"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-34c5301f474c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"Weights-LSTM-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=80, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163817\n",
      "Total Vocab:  61\n",
      "Total Patterns:  163717\n",
      "Seed:\n",
      "\" ,\n",
      "to keep herself from being run over; and the moment she appeared on the\n",
      "other side, the puppy made \"\n",
      " ont all the same thmence. \n",
      "‘i was a conm was,’ said the daterpillar.\n",
      "\n",
      "‘well, i should laver bod mone to the same thing,’ said the daterpillar.\n",
      "\n",
      "‘well, i’ve tried the reme than ’ said the daterpillar.\n",
      "\n",
      "‘well, i’ve tried the reme than ’ sa"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-ed2fa0f0f746>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1006\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1788\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[1;32m-> 1790\u001b[1;33m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1297\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"Weights-LSTM-improvement-44-1.3592-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(700):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
